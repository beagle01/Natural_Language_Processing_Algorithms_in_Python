{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram\n",
    "\n",
    "Maximum likelihood estimation (MLE)\n",
    "\n",
    "MLE of Unigram\n",
    "\n",
    "\n",
    "Unknow word probability\n",
    "\n",
    "Likelihood: The probabilities of some observation data in the test set Wtest, given the model M.\n",
    "\n",
    "LogLikelihood\n",
    "\n",
    "Entropy (H) is the average negative log2Likelihood per word.  Perplexity (PPL) is two to the power of per-word entropy.\n",
    "\n",
    "Coverage is the percentage of known words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a Unigram model on the training data\n",
    "\n",
    "counts = {}\n",
    "total_count = 0\n",
    "\n",
    "#load training data and count words\n",
    "train_lines = open(\"data/wiki-en-train.txt\", \"r\").readlines()\n",
    "for line in train_lines:\n",
    "    words = line.strip().split(\" \")\n",
    "    words.append(\"</s>\")\n",
    "    for word in words:\n",
    "        if word in counts.keys():\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "        total_count += 1\n",
    "\n",
    "#calculate the probability of each unique word in the training data, and save the model\n",
    "model_unigram = open(\"data/model_unigram.txt\", \"w\")\n",
    "for word, count in counts.items():\n",
    "    probability = count / total_count\n",
    "    model_unigram.write(word + \" \" + str(probability) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 11.927857382884122\n",
      "Coverage = 0.8248423095584667\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#load model\n",
    "probabilities = {}\n",
    "model_unigram = open(\"data/model_unigram.txt\", \"r\").readlines()\n",
    "for line in model_unigram:\n",
    "    w_p = line.strip().split(\" \")\n",
    "    probabilities[w_p[0]] = float(w_p[1])\n",
    "\n",
    "#set parameters\n",
    "lamda_1 = 0.95\n",
    "lamda_unk = 1 - lamda_1\n",
    "V = 1000000\n",
    "word_unk = 0\n",
    "W = 0\n",
    "H = 0\n",
    "\n",
    "#calculate entropy and coverage on test data\n",
    "test_lines = open(\"data/wiki-en-test.txt\", \"r\").readlines()\n",
    "for line in test_lines:\n",
    "    words = line.strip().split(\" \")\n",
    "    words.append(\"</s>\")\n",
    "    for word in words:\n",
    "        W += 1\n",
    "        P = lamda_unk / V\n",
    "        if word in probabilities.keys():\n",
    "            P += lamda_1 * probabilities[word]\n",
    "        else:\n",
    "            word_unk += 1\n",
    "        H += -(math.log2(P))\n",
    "\n",
    "print(\"Entropy =\", H / W)\n",
    "print(\"Coverage =\", (W - word_unk) / W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
