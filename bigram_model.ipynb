{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram\n",
    "\n",
    "Maximum Likelihood Estimation of n-gram Probability\n",
    "\n",
    "Context Dependent Smoothing\n",
    "\n",
    "Witten-Bell Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "train_lines = open(\"data/wiki-en-train.txt\", \"r\").readlines()\n",
    "\n",
    "#calculate bigram and context counts on training data\n",
    "counts = {} #bigram and unigram counts\n",
    "context_counts = {} #bigram context and unigram context counts\n",
    "\n",
    "for line in train_lines:\n",
    "    words = line.strip().split(\" \")\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "    for i in range(1, len(words) - 1):\n",
    "        #count bigram\n",
    "        if words[i-1] + \" \" + words[i] in counts.keys():\n",
    "            counts[words[i-1] + \" \" + words[i]] += 1\n",
    "        else:\n",
    "            counts[words[i-1] + \" \" + words[i]] = 1\n",
    "\n",
    "        #count bigram context\n",
    "        if words[i-1] in context_counts.keys():\n",
    "            context_counts[words[i-1]] += 1\n",
    "        else:\n",
    "            context_counts[words[i-1]] = 1\n",
    "\n",
    "        #count unigram\n",
    "        if words[i] in counts.keys():\n",
    "            counts[words[i]] += 1\n",
    "        else:\n",
    "            counts[words[i]] = 1\n",
    "\n",
    "        #count unigram context\n",
    "        if \"\" in context_counts.keys():\n",
    "            context_counts[\"\"] += 1\n",
    "        else:\n",
    "            context_counts[\"\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find context of ngram, calculate the probabilities of ngram, and save the ngram model\n",
    "bigram_model = open(\"data/bigram_model.txt\", \"w\")\n",
    "for ngram, count in counts.items():\n",
    "    words = ngram.split(\" \")\n",
    "    words.pop()\n",
    "    context = \" \".join(words) #context of ngram\n",
    "    probability = count / context_counts[context] #calculate MLE of ngram probabilities\n",
    "    bigram_model.write(ngram + \"======>\" + str(probability) + \"\\n\") #save the ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 12.552196580782692\n",
      "Coverage = 0.3469903894790086\n"
     ]
    }
   ],
   "source": [
    "#calculate entropy and coverage on the test set using context dependent smoothing\n",
    "\n",
    "import math\n",
    "\n",
    "#load ngram model and convert to a dictionary {ngram: probability}\n",
    "model_bigram = open(\"data/bigram_model.txt\", \"r\").readlines()\n",
    "probabilities = {}\n",
    "for line in model_bigram:\n",
    "    ngram_p = line.strip().split(\"======>\")\n",
    "    probabilities[ngram_p[0]] = float(ngram_p[1])\n",
    "\n",
    "#set parameters\n",
    "lamda_1 = 0.85 #unknown unigram probability is 1 - lamda_1\n",
    "lamda_2 = 0.85 #unknown bigram probability is 1 - lamda_2\n",
    "V = 1000000 #guess total vocabulary size\n",
    "W_unk = 0 #initiate the counts of unknown ngrams\n",
    "W = 0 #initiate the counts of all ngrams\n",
    "H = 0 #initiate the negative log2 likelihood\n",
    "\n",
    "#load test data\n",
    "test_lines = open(\"data/wiki-en-test.txt\", \"r\").readlines()\n",
    "for line in test_lines:\n",
    "    words = line.strip().split(\" \")\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "    for i in range(1, len(words) -1):\n",
    "        if words[i-1] + \" \" + words[i] not in probabilities.keys():\n",
    "            W_unk += 1\n",
    "            #smoothing unigram probability\n",
    "            if words[i] not in probabilities.keys():\n",
    "                P1 = (1 - lamda_1) / V \n",
    "            else:\n",
    "                P1 = lamda_1 * probabilities[words[i]] + (1 - lamda_1) / V\n",
    "            #smoothing bigram probability\n",
    "            P2 = (1 - lamda_2) * P1\n",
    "        else:\n",
    "            #smoothing unigram probability\n",
    "            if words[i] not in probabilities.keys():\n",
    "                P1 = (1 - lamda_1) / V \n",
    "            else:\n",
    "                P1 = lamda_1 * probabilities[words[i]] + (1 - lamda_1) / V\n",
    "            #smooting bigram probability\n",
    "            P2 = lamda_2 * probabilities[words[i-1]+ \" \" + words[i]] + (1 - lamda_2) * P1\n",
    "        H += - (math.log2(P2))\n",
    "        W += 1\n",
    "        \n",
    "print(\"Entropy =\", H / W)\n",
    "print(\"Coverage =\", (W - W_unk) / W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gram probabilities (lambda) by Witten-Bell Smoothing\n",
    "\n",
    "def lamda(test_set):\n",
    "    counts = {} #count ngram\n",
    "    counts_W = {} #count unigram\n",
    "    counts_U = {} #count unique ngram\n",
    "    lamda_dict = {}\n",
    "\n",
    "    test_lines = open(test_set, \"r\").readlines()\n",
    "\n",
    "    for line in test_lines:\n",
    "        words = line.strip().split(\" \")\n",
    "        words.insert(0, \"<s>\")\n",
    "        words.append(\"</s>\")\n",
    "        for i in range(1, len(words)):\n",
    "            #check unique ngram\n",
    "            if words[i-1] + \" _\" not in counts_U.keys():\n",
    "                counts_U[words[i-1] + \" _\"] = 0\n",
    "            #find and count unigram\n",
    "            if words[i-1] in counts_W.keys():\n",
    "                counts_W[words[i-1]] += 1\n",
    "            else:\n",
    "                counts_W[words[i-1]] = 1\n",
    "            #find and count ngram\n",
    "            if words[i-1] + \" \" + words[i] in counts.keys():\n",
    "                counts[words[i-1] + \" \" + words[i]] += 1\n",
    "            else:\n",
    "                counts[words[i-1] + \" \" + words[i]] = 1\n",
    "                counts_U[words[i-1] +\" _\"] += 1 #add unique ngram\n",
    "\n",
    "    for w, c in counts_W.items():\n",
    "        lamda_dict[w] = 1 - (counts_U[w + \" _\"] / (counts_U[w + \" _\"] + c))\n",
    "\n",
    "    return lamda_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 11.652420370041423\n",
      "Coverage = 0.3469903894790086\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#load bigram model\n",
    "model_bigram = open(\"data/bigram_model.txt\", \"r\").readlines()\n",
    "probabilities = {}\n",
    "for line in model_bigram:\n",
    "    ngram_p = line.strip().split(\"======>\")\n",
    "    probabilities[ngram_p[0]] = float(ngram_p[1])\n",
    "\n",
    "#set parameters\n",
    "lamda = lamda(\"data/wiki-en-test.txt\") #gram probabilities (lambda of gram)\n",
    "V = 1000000 #guess total vocabulary size\n",
    "W_unk = 0 #initiate the counts of unknown ngrams\n",
    "W = 0 #initiate the counts of all ngrams\n",
    "H = 0 #initiate the negative log2 likelihood\n",
    "\n",
    "#load test data\n",
    "test_lines = open(\"data/wiki-en-test.txt\", \"r\").readlines()\n",
    "for line in test_lines:\n",
    "    words = line.strip().split(\" \")\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "    for i in range(1, len(words) -1):\n",
    "        if words[i-1] + \" \" + words[i] not in probabilities.keys():\n",
    "            W_unk += 1\n",
    "            if words[i] not in probabilities.keys():\n",
    "                P1 = (1 - lamda[words[i]]) / V \n",
    "            else:\n",
    "                P1 = lamda[words[i]] * probabilities[words[i]] + (1 - lamda[words[i]]) / V\n",
    "            P2 = (1 - lamda[words[i-1]]) * P1\n",
    "        else:\n",
    "            if words[i] not in probabilities.keys():\n",
    "                P1 = (1 - lamda[words[i]]) / V \n",
    "            else:\n",
    "                P1 = lamda[words[i]] * probabilities[words[i]] + (1 - lamda[words[i]]) / V\n",
    "            P2 = lamda[words[i-1]] * probabilities[words[i-1]+ \" \" + words[i]] + (1 - lamda[words[i-1]]) * P1 #smoothed bigram probability\n",
    "        H += - (math.log2(P2))\n",
    "        W += 1\n",
    "        \n",
    "print(\"Entropy =\", H / W)\n",
    "print(\"Coverage =\", (W - W_unk) / W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
